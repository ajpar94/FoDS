{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 7: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "    \n",
    "    \n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = None   # weights of the model\n",
    "    \n",
    "    def loss(self,X,y,lam=0): # TODO: add arguments if needed\n",
    "        \"\"\" Computes empirical risk\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        m,n = X.shape\n",
    "        X0 = np.c_[np.ones(m),X]\n",
    "        h = 1/(1+np.exp(-X0 @ self.w))\n",
    "        l = -1/m*(y @ np.log(h) + (1-y) @ np.log(1-h)) + lam/(2*m) * (self.w.T @ self.w)\n",
    "        return l\n",
    "\n",
    "    def fit(self, X,y,eta, t, lam=0, plot=True, epsilon=1e-7): # TODO: add arguments if needed\n",
    "        \"\"\" Fit linear logistic regression model \n",
    "            Input:\n",
    "                X:  2-dim binary Array of shape (m,n), \n",
    "                    where m is the number of examples and n is the dimension of the data\n",
    "                y:  Array of length m containing the class label of each example in X\n",
    "            Output: \n",
    "                LOSS:   Array or list of loss values: \n",
    "                        LOSS[i] contains the loss after the update in iteration i\n",
    "                        LOSS[0] is the loss of the initial model\n",
    "                ACCURACY: Array or list of accuracy values: \n",
    "                eta:  learning rate\n",
    "                t: maximun iteration times\n",
    "                lam: L2-regulation parameter\n",
    "        \"\"\"\n",
    "        self.w = np.array([0.5,0.5,0.5])\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        m,n = X.shape\n",
    "        X0 = np.c_[np.ones(m),X]\n",
    "        LOSS = []\n",
    "        LOSS.append(self.loss(X,y,lam))\n",
    "        ACCURACY = []\n",
    "        ACCURACY.append(1 - np.mean(abs(self.predict(X)-y)))          \n",
    "        for _ in range(t):\n",
    "            h = 1/(1+np.exp(-X0 @ self.w))                 \n",
    "            # calculate gradient\n",
    "            gradient = ((X0.T @ (h - y))+ lam*self.w)/m      \n",
    "            #check termination criteria\n",
    "            #if (np.linalg.norm(eta * gradient) < epsilon):\n",
    "                #break\n",
    "            #eta *= 0.9\n",
    "            self.w -= eta * gradient\n",
    "            LOSS.append(self.loss(X,y,lam))\n",
    "            ACCURACY.append(1 - np.mean(abs(self.predict(X)-y)))\n",
    "        \n",
    "        # TODO: find weights by gradient descent. compute loss and accuracy after each update \n",
    "        #       and append it to the lists LOSS and ACCURACY.\n",
    "        #       Store fitted weights in self.w\n",
    "        print(self.w)\n",
    "        return LOSS,ACCURACY\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" compute predicted labels\n",
    "            Input:\n",
    "                X:         Data matrix, format as described in fit()\n",
    "            Output: \n",
    "                y_pred:    list of predicted labels\n",
    "                \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        X = np.array(X)\n",
    "        m,n = X.shape\n",
    "        X0 = np.c_[np.ones(m),X]\n",
    "        h = 1/(1+np.exp(-X0 @ self.w))\n",
    "        y_pred = np.round(h)\n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\" Evaluates the fitted model on test data X,y\n",
    "            Input:\n",
    "                X: Data matrix, format as described in fit()\n",
    "                y: Labels, format as described in fit()\n",
    "            Returns:\n",
    "                acc: Accuracy\n",
    "        \"\"\"\n",
    "        prediction = self.predict(X)\n",
    "        acc = sum(prediction == y) / len(y)\n",
    "        return acc\n",
    "    \n",
    "    ### TODO: Exercise 7.5.2  ###\n",
    "    def most_sure(self, X, y):\n",
    "        \"\"\" Finds and plots the example in X for which the classifier is most sure\n",
    "        \"\"\"\n",
    "        probability = None\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "        x = None\n",
    "        print('Most sure prediction: probability = ',probability, '   true label = ',y_true, '   predicted label = ',y_pred )\n",
    "        plot_mnist(x)\n",
    "        \n",
    "    ### TODO: Exercise 7.5.2  ### \n",
    "    def least_sure(self, X, y):\n",
    "        \"\"\" Finds and plots the example in X for which the classifier is least sure (most unsure).\n",
    "        \"\"\"\n",
    "        probability = None\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "        x = None\n",
    "        print('Least sure prediction: probability = ',probability, '   true label = ',y_true, '   predicted label = ',y_pred )\n",
    "        plot_mnist(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.166679    0.74576935 -1.68588888]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def get_toy_data():\n",
    "    n = 100\n",
    "    X1 = np.random.multivariate_normal([1,2],[[1, 0],[0, 2]], n)\n",
    "    X2 = np.random.multivariate_normal([10,-5],[[1, 0.2],[0.2, 2]], n)\n",
    "    X = np.concatenate((X1,X2),axis=0)\n",
    "    y = np.array([0]*n + [1]*n)\n",
    "    return X,y\n",
    "\n",
    "X,y = get_toy_data()\n",
    "\n",
    "\n",
    "colors = ('red', 'blue')\n",
    "cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "xx1, xx2 = np.meshgrid(np.arange(-2, 14, 0.01),np.arange(-10, 8, 0.01))\n",
    "x = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "\n",
    "\n",
    "classifier1 = LogisticRegression()\n",
    "classifier2 = LogisticRegression()\n",
    "\n",
    "# TODO:\n",
    "# train classifier without regularization\n",
    "LOSS0,ACCURACY0 = classifier1.fit(X,y,1,1000, lam=0, plot=True, epsilon=1e-7)\n",
    "y0 = classifier1.predict(x).reshape(xx1.shape)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"without regularization\", size=20, fontweight='bold')\n",
    "plt.contourf(xx1,xx2, y0,alpha=0.3,cmap=cmap)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap=cmap)\n",
    "\n",
    "# train classifier with regularization\n",
    "LOSS1,ACCURACY1 = classifier2.fit(X,y,1,1000, lam=0.1, plot=True, epsilon=1e-7)\n",
    "y1 = classifier2.predict(x).reshape(xx1.shape)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"with regularization\", size=20, fontweight='bold')\n",
    "plt.contourf(xx1,xx2, y0,alpha=0.3,cmap=cmap)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap=cmap)\n",
    "\n",
    "# plot ACCURACY and LOSS curves for both models\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"ACCURACY (x axis scaled logarithmically)\", size=20, fontweight='bold')\n",
    "plt.xlabel(\"time t\",size=20)\n",
    "plt.ylabel(\"ACCURACY\",size=20)\n",
    "plt.semilogx(ACCURACY0,label= \"lamda = 0\")\n",
    "plt.semilogx(ACCURACY1,label= \"lamda = 0.1\")\n",
    "plt.ylim(0.88,1.01)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid() \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"LOSS( x axis scaled logarithmically)\", size=20, fontweight='bold')\n",
    "plt.xlabel(\"time t\",size=20)\n",
    "plt.ylabel(\"LOSS\",size=20)\n",
    "plt.semilogx(LOSS0,label= \"lamda = 0\" )\n",
    "plt.semilogx(LOSS1,label= \"lamda = 0.1\" )\n",
    "plt.ylim(0,0.25)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid() \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_mnist(path, classes = range(10)):\n",
    "    \"\"\" Load data from mnist dataset stored in csv format. \n",
    "        Rows correspond to examples.\n",
    "        First column are the labels, all other columns constitute the data.\n",
    "        Each image is 28 x 28 stored as 784-dim. vector .\n",
    "        Input: \n",
    "            path      path to csv file\n",
    "            classes   list of classes to extract\n",
    "        Output:\n",
    "            data      design matrix holding the images\n",
    "            labels    array of corresponding labels\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    lab = np.array(df.iloc[:,0])\n",
    "    dat = np.array(df.iloc[:,1:])\n",
    "\n",
    "    ind = [c in classes for c in lab]\n",
    "\n",
    "    data = dat[ind,:]\n",
    "    labels = lab[ind]\n",
    "\n",
    "    return data,labels\n",
    "\n",
    "def plot_mnist(x):\n",
    "    \"\"\" Plot an mnist image.\n",
    "        Input:\n",
    "            x      784-dim. vector representing a grayscale image of size 28 x 28.\n",
    "    \"\"\"\n",
    "    img = np.reshape(x,(28,28))\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# load data\n",
    "data_test, labels_test = load_mnist('mnist_test.csv', classes = [2,9])\n",
    "data_train, labels_train = load_mnist('mnist_train.csv', classes = [2,9])\n",
    "\n",
    "# TODO: further steps if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-37b74af9ad87>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-37b74af9ad87>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    plt.plot(LOSS, label='loss')\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# execute to run Exercise 7.5 # \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "LOSS,ACC = classifier.fit(data_train,labels_train,...# TODO: add parameters)\n",
    "plt.plot(LOSS, label='loss')\n",
    "plt.plot(ACC, label='accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "                      \n",
    "acc_test = classifier.evaluate(data_test,labels_test)\n",
    "acc_train = classifier.evaluate(data_train,labels_train)\n",
    "print('Test Accuracy =',acc_test)\n",
    "print('Train Accuracy =',acc_train)\n",
    "\n",
    "classifier.most_sure(data_test,labels_test)\n",
    "classifier.least_sure(data_test,labels_test)\n",
    "\n",
    "# Exercise 7.5.3\n",
    "w = classifier.w\n",
    "# TODO: plot weights as image using plot_mnist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
